{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Basic filtering of the entries\n",
    "MIN_YEAR = 1980 # Ignore all older occurrences\n",
    "COUNT_THRESHOLD = 500 # Ignore all lesser occurrences\n",
    "\n",
    "# Template of the regular expression specifying which entries to keep\n",
    "KEEP_TEMPLATE = ur\"^%s_DET [a-záàâäãåçéèêëíìîïñóòôöõúùûüýÿæœ]*?_NOUN$\"\n",
    "\n",
    "# Where and under which names the required Google Ngrams datasets can be found\n",
    "DIRECTORY = os.path.expanduser(\"~/Downloads\")\n",
    "NAME_TEMPLATE = \"googlebooks-fre-all-2gram-20120701-%s.gz\"\n",
    "\n",
    "# The  CSV files are read by chunks of rows\n",
    "# WARNING: changing this value may raise an error\n",
    "# (Pandas issue: https://github.com/pydata/pandas/issues/5291)\n",
    "CHUNK_SIZE = 99999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None):\n",
    "    \"\"\"Widget based progress bar for Jupyter (IPython Notebook)\n",
    "    \n",
    "    Author: Kukushkin Alexander\n",
    "    Source: https://github.com/alexanderkuk/log-progress\n",
    "    \"\"\"\n",
    "\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = size / 200     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{index} / ?'.format(index=index)\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{index} / {size}'.format(\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = str(index or '?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If no HTML logger is available, uncomment the following lines\n",
    "\n",
    "# def log_progress(sequence, every, size):\n",
    "#     \"\"\"Fallback in case the program is run outside Jupyter Notebook.\"\"\"\n",
    "#     for (i, element) in enumerate(sequence, 1):\n",
    "#         if i % every == 0:\n",
    "#             print \"%s/%s\" % (i, size)\n",
    "#         yield element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Description of the Google Ngrams datasets\n",
    "\n",
    "sources = [\n",
    "    {\n",
    "        \"name\": \"le\", # the discriminant part of the filename\n",
    "        \"keep\": \"[Ll]e\", # sub-regexp for the lines to keep\n",
    "        \"discriminant\": 1, # position of a character marking the gender\n",
    "        \"masc\": \"e\", # value of this character for the masculine gender\n",
    "        \"masc_start\": 7, # first position of a masculine noun\n",
    "        \"row_count\": 249600630\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"la\",\n",
    "        \"keep\": \"[Ll]a\",\n",
    "        \"discriminant\": 1,\n",
    "        \"masc\": \"e\", # no occurrence\n",
    "        \"fem_start\": 7, # first position of a feminine noun\n",
    "        \"row_count\": 156511945\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"du\",\n",
    "        \"keep\": \"[Dd]u\",\n",
    "        \"discriminant\": 1,\n",
    "        \"masc\": \"u\", # any occurrence\n",
    "        \"masc_start\": 7,\n",
    "        \"row_count\":  67674376\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"un\",\n",
    "        \"keep\": \"[Uu]ne?\",\n",
    "        \"discriminant\": 2,\n",
    "        \"masc\": \"_\", # discriminate \"un_\" against \"une_\"\n",
    "        \"masc_start\": 7,\n",
    "        \"fem_start\": 8,\n",
    "        \"row_count\":  89455794\n",
    "    },\n",
    "]\n",
    "dtype = OrderedDict([\n",
    "    (\"digram\", unicode),\n",
    "    (\"year\", np.int16),\n",
    "    (\"match_count\", np.int32),\n",
    "    (\"volume_count\", np.int32),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "googlebooks-fre-all-2gram-20120701-le.gz\n",
      "googlebooks-fre-all-2gram-20120701-la.gz\n",
      "googlebooks-fre-all-2gram-20120701-du.gz\n",
      "googlebooks-fre-all-2gram-20120701-un.gz\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Main program\n",
    "\n",
    "result = pd.DataFrame()\n",
    "\n",
    "for source in sources:\n",
    "    \n",
    "    # Define an iterator on the current CSV file by chunks of CHUNK_SIZE\n",
    "    filename = NAME_TEMPLATE % source[\"name\"]\n",
    "    print filename\n",
    "    iter_csv = pd.read_csv(\n",
    "        os.path.join(DIRECTORY, filename),\n",
    "        encoding=\"utf8\",\n",
    "        sep=\"\\t\",\n",
    "        iterator=True,\n",
    "        chunksize=CHUNK_SIZE,\n",
    "        header=None,\n",
    "        names=dtype.keys(),\n",
    "        dtype=dtype\n",
    "    )\n",
    "    \n",
    "    # Construct chunk by chunk  the part corresponding to the current file\n",
    "    part = pd.DataFrame()\n",
    "    size = int(math.ceil(source[\"row_count\"] / CHUNK_SIZE))\n",
    "    for chunk in log_progress(iter_csv, every=1, size=size):\n",
    "        # Filter the digrams\n",
    "        part = part.append(chunk[\n",
    "                  (chunk[\"year\"] >= MIN_YEAR)\n",
    "                & chunk[\"digram\"].str.match(KEEP_TEMPLATE % source[\"keep\"])\n",
    "            ])\n",
    "        \n",
    "    # Aggregate the occurrence counts by digram\n",
    "    part = part.groupby(\"digram\").agg({\"match_count\": \"sum\"})\n",
    "    part.reset_index(inplace=True)\n",
    "    \n",
    "    # Break down the digrams into the corresponding gender and noun\n",
    "    part[\"gender\"] = np.where(part[\"digram\"].str.get(source[\"discriminant\"]) == source[\"masc\"], \"m\", \"f\")\n",
    "    part[\"noun\"] = np.where(\n",
    "        part[\"gender\"] == \"m\",\n",
    "        part[\"digram\"].str.slice(source.get(\"masc_start\"), -len(\"_NOUN\")),\n",
    "        part[\"digram\"].str.slice(source.get(\"fem_start\"), -len(\"_NOUN\"))\n",
    "    )\n",
    "    part.drop(\"digram\", axis=1, inplace=True)\n",
    "    \n",
    "    # Accumulate the current part\n",
    "    result = result.append(part)\n",
    "\n",
    "# Aggregate the occurrence counts by noun and gender\n",
    "result = result.groupby([\"noun\", \"gender\"]).agg({\"match_count\": \"sum\"})\n",
    "result.reset_index(inplace=True)\n",
    "result.rename(columns={\"match_count\": \"count\"}, inplace=True)\n",
    "\n",
    "# Suppress the nouns having less than COUNT_THRESHOLD occurrences\n",
    "result = result[result[\"count\"] >= COUNT_THRESHOLD]\n",
    "\n",
    "# Reorder the columns\n",
    "result = result[[\"noun\", \"gender\", \"count\"]]\n",
    "\n",
    "# Sort the resulting table by decreasing number of occurrences\n",
    "result.sort(\"noun\", inplace=True)\n",
    "\n",
    "# Write the result\n",
    "result.to_csv(\"french_nouns.tsv\", sep='\\t', encoding='utf-8', index=False)\n",
    "print \"Done.\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
